О! Привет, Артем! Классно, что ты мой ревьюер!

Ты прав. Поменял триггер. Запустил скрипт, потом залил данные и понял как работает потоковая обработка данных)
Заодно увидел своими глазами, что такое "протухнуть сообщению" в kafka
По приведению данных к определенному типу решил, что лучше текущее время привести к Unix, как мне кажется обработка числовых значений много ьыстрее, чем работа с timestampType
За ликбез спасибо)


В целом многое получилось, хоть и несколько коряво
Единственная проблема, которую не могу решить это обработка foreachBatch и соответсвенно реализация foreach_batch_function
1. Из документации нет ясности, что такое epoch_id и как его задействовать в реализации программы

2. Если я правильно понимаю, то создать таблицу в PostgreSQL с помощью спарк сессии невозможно или гуглу это не доступно
Поэтому в этой части скрипт написан на простом psycopg2
Я видел примеры, когда данные просто передавались в несуществующую таблицу и она создавалась в моменте, но правильно ли это с точки зрения конечных данных

3. Поле feedback как я понимаю пустое и передается пустым (не передается)

4. И самое главное: когда мы вызываем foreach_batch_function после отправки данных в postgresql мы отправляем их также в kafka
как правильно делать это с option foreachBatch именно в кафку я не нашел и в уроке, увы этого не было, подскажи, пожалуйста, как в самой функции выполнить отправку в kafka
В том исполнении, как это сделано сейчас я получаю ошибку pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame
Как будто это не потоковый DataFrame и к нему нельзя применять метод writeStream


Сорри, что отправляю в пятницу, если занят - не проблема, еще есть куча дел)